title: 实时策略
date: 2013-12-18 
category:技术


传统的推荐以离线为主， 以预测用户偏好品牌为例，大致分为这几个步骤:

- 离线采集计算用户， 品牌等各个维度的特征。
- 根据业务目标给出样本集合， 比如比较典型的CTR， 对于给定样本数据， 点击为正例， 不点击为负例。候选集的挑选有时候充满了trick， 尤其在业务的主体流量不是推荐的时候。
- 使用模型进行离线训练， 以线性模型居多， 包括LR, pair-wise, list-wise等。在海量情况下， 复杂模型比较罕见。
- 根据模型预测获得用户的偏好品牌列表。

相对于直接利用热门或者协同过滤做出的推荐， 这种方式能够获得良好的提升以及更好的泛化能力， 但是面对以下几种情况无能为力：

* 每天有大量新用户拥入。
* 用户对品牌的点击在过去没有特征， 看起来似乎是天外飞仙般出现， 但又不是热销。 
* 无法实时的对用户的正反馈和负反馈进行算法上的回馈， 尤其是当你发现你X掉了一堆东西， 类似的商品还在源源不断的涌向你的时候。

实时的数据对于推荐系统而言， 是一座新的金矿， 可以为用户进行更准确地推荐， 同时及时响应各种反馈， 提升用户体验。 用户能感受到推荐系统和他的交互， 会更主动积极的贡献自己的行为特点， 从而形成良性循环。


比较常见的一种做法是实时特征的引入， 训练过程依然在离线完成：

1. 收集实时数据， 做离线训练
2. 做在线预测

另一部分采用的是online learning, 利用用户不停的行为反馈（包含显式的和隐式的）， 来调整特征权重。

现在在考虑一种在线增强学习的方式， 假定我们现在有四种投放策略：a, b, c, d。
对于缺乏历史信息的新用户， 在没有任何信息的情况下， 我们先以轮播的方式在所有的位置投放a, b, c, d四种策略。
随后我们采集相应的反馈信息：

* 点击：显式正反馈
* 删除： 显式负反馈
* 未点击： 隐式负反馈

在这一过程中快速积累用户对不同策略的敏感程度，同时对离线的预测结果进行纠正。

由于单独位置对用户的曝光机会有限， 需要以打通的眼光看待所有的投放位置， 将他作为一个整体：

1. 收集到显式负反馈的策略在全局降分
2. 收集到显式负反馈的目标在全局过滤

最轻量级的快速尝试方式是， 对于某部分用户而言， 预估的投放策略可信度已经低于热门了， 或者补全的时候采用相似算法的可信度已经低于热门了， 就可以通过快速切换策略达到在线选择的目的。

对于实时而言， 不仅仅是算法上的变更， 对系统架构也有巨大的挑战。
    下图是Netflix的实时推荐的架构图：
"![Netflix架构](http://orange-tree.oss-cn-hangzhou.aliyuncs.com/blog-images/online系统.jpg)"

系统上， Netflix分为online, near online, offline三个部分， nearline是用户最近的行为数据， 利用流失计算获得一些结果， 产生的结果送到online 用以更新模型。

